{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 1: Preprocess Data<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Feature_extraction/train_set_features_4_fruits.pkl\", \"rb\") as f:\n",
    "    train_set_features2 = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/train_set_labels_4_fruits.pkl\", \"rb\") as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/test_set_features_4_fruits.pkl\", \"rb\") as f:\n",
    "    test_set_features2 = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/test_set_labels_4_fruits.pkl\", \"rb\") as f:\n",
    "    test_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=train_set_features2, axis=0)\n",
    "train_set_features = train_set_features2[:, features_STDs > 52.3]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
    "\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=test_set_features2, axis=0)\n",
    "test_set_features = test_set_features2[:, features_STDs > 48]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "test_set_features = np.divide(test_set_features, test_set_features.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i in range(len(train_set_features)):\n",
    "    label = np.zeros((4, ), dtype='int32')\n",
    "    label[int(train_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    train_set.append((train_set_features[i].reshape(102,1), label))\n",
    "    \n",
    "\n",
    "for i in range(len(test_set_features)):\n",
    "    label = np.zeros((4, ), dtype='int32')\n",
    "    label[int(test_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    test_set.append((test_set_features[i].reshape(102,1), label))\n",
    "    \n",
    "# shuffle\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_features.shape)\n",
    "print(train_set_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 2: Forward Propagation<h2/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims): \n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) \n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "          \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, parameters):\n",
    "    middleputs = {}\n",
    "    A = x\n",
    "    \n",
    "    for l in range(len(parameters) // 2):\n",
    "        middleputs['Z'+str(l+1)] = np.matmul(parameters['W'+str(l+1)], A) + parameters['b'+str(l+1)]\n",
    "        middleputs['A'+str(l+1)] = sigmoid(middleputs['Z'+str(l+1)])\n",
    "        A = middleputs['A'+str(l+1)]\n",
    "    \n",
    "    return A, middleputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, parameters):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for feature, label in dataset:\n",
    "        output, _ = forward(feature, parameters)\n",
    "        \n",
    "        predicted = np.argmax(output, axis=0).item()\n",
    "        target = np.argmax(label, axis=0).item()\n",
    "        \n",
    "        if predicted == target:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "        \n",
    "    return {'accuracy':float('{:.2f}'.format(100 * (correct / total)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([102, 150, 60, 4])\n",
    "test(train_set[:200], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 3: Non-Vectorized Backward Propagetion<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_se(predicted, target):\n",
    "    return np.sum(np.square(predicted - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size: 17px'><em>warning: before running the below cell, please run phase 7 code block<em/><p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size, lr, dataset, layer_dims, backward_func, lr_decay_weight=0):\n",
    "    \n",
    "    dataset = deepcopy(dataset)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    costs = []\n",
    "    lr_scheduler = LR_Decay(lr, lr_decay_weight)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), leave=False, bar_format='{percentage:02.0f}%  {bar}  epoch {n_fmt}/{total_fmt}'):\n",
    "        random.shuffle(dataset)\n",
    "        cost = 0\n",
    "        epoch_lr=lr_scheduler.step()\n",
    "        \n",
    "        batch_num = len(dataset) // batch_size\n",
    "        if len(dataset) % batch_size != 0:\n",
    "            batch_num += 1\n",
    "            \n",
    "        for i in range(batch_num):\n",
    "            batch = dataset[i * batch_size:min((i * batch_size) + batch_size, len(dataset))]\n",
    "            \n",
    "            grads = {}\n",
    "            for l in range(len(parameters) // 2):\n",
    "                grads['dW'+str(l+1)] = np.zeros_like(parameters['W'+str(l+1)])\n",
    "                grads['db'+str(l+1)] = np.zeros_like(parameters['b'+str(l+1)])\n",
    "                \n",
    "            \n",
    "            for x, y in batch:\n",
    "                output, middleputs = forward(x, parameters)\n",
    "                s_cost = cost_se(output, y)\n",
    "                s_grads = backward_func(x, y, parameters, middleputs)\n",
    "                \n",
    "                cost += s_cost\n",
    "                \n",
    "                for l in range(len(parameters) // 2):\n",
    "                    grads['dW'+str(l+1)] = grads['dW'+str(l+1)] + s_grads['dW'+str(l+1)]\n",
    "                    grads['db'+str(l+1)] = grads['db'+str(l+1)] + s_grads['db'+str(l+1)]\n",
    "               \n",
    "            \n",
    "            for l in range(len(parameters) // 2):\n",
    "                parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - (epoch_lr * (grads['dW'+str(l+1)] / batch_size))\n",
    "                parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - (epoch_lr * (grads['db'+str(l+1)] / batch_size))\n",
    "        \n",
    "        costs.append(cost / len(dataset))\n",
    "    \n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonvectorized_backward(x, label, parameters, middleput):\n",
    "    \n",
    "    l = len(parameters) // 2\n",
    "    grads = {}\n",
    "    \n",
    "    # last layer \n",
    "    b_shape = parameters['b'+str(l)].shape\n",
    "    \n",
    "    dA = np.zeros(b_shape)\n",
    "    for i in range(dA.shape[0]):\n",
    "        dA[i, 0] += 2 * (middleput['A'+str(l)][i, 0] - label[i, 0])\n",
    "    \n",
    "    W_shape= parameters['W'+str(l)].shape\n",
    "    grads['dW'+str(l)] = np.zeros(W_shape)\n",
    "    for i in range(W_shape[0]):\n",
    "        for j in range(W_shape[1]):\n",
    "            grads['dW'+str(l)][i, j] = middleput['A'+str(l-1)][j, 0] * sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "\n",
    "    grads['db'+str(l)] = np.zeros(b_shape)\n",
    "    for i in range(b_shape[0]):\n",
    "        grads['db'+str(l)][i, 0] = sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "\n",
    "    l -= 1\n",
    "    \n",
    "    # hidden layers\n",
    "    while l > 1:\n",
    "        dA_prev = dA\n",
    "        b_shape = parameters['b'+str(l)].shape\n",
    "    \n",
    "        dA = np.zeros(b_shape)\n",
    "        for i in range(dA.shape[0]):\n",
    "            for j in range(dA_prev.shape[0]):\n",
    "                dA[i, 0] += parameters['W'+str(l+1)][j, i] * sigmoid_deriv(middleput['Z'+str(l+1)][j, 0]) * dA_prev[j, 0]\n",
    "        \n",
    "        W_shape = parameters['W'+str(l)].shape\n",
    "        grads['dW'+str(l)] = np.zeros(W_shape)\n",
    "        for i in range(W_shape[0]):\n",
    "            for j in range(W_shape[1]):\n",
    "                grads['dW'+str(l)][i, j] = middleput['A'+str(l-1)][j, 0] * sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "        \n",
    "        grads['db'+str(l)] = np.zeros(b_shape)\n",
    "        for i in range(b_shape[0]):\n",
    "            grads['db'+str(l)][i, 0] = sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "            \n",
    "        l -= 1\n",
    "\n",
    "    # first layer\n",
    "    dA_prev = dA\n",
    "    b_shape = parameters['b'+str(l)].shape\n",
    "\n",
    "    dA = np.zeros(b_shape)\n",
    "    for i in range(dA.shape[0]):\n",
    "        for j in range(dA_prev.shape[0]):\n",
    "            dA[i, 0] += parameters['W'+str(l+1)][j, i] * sigmoid_deriv(middleput['Z'+str(l+1)][j, 0]) * dA_prev[j, 0]\n",
    "\n",
    "    W_shape = parameters['W'+str(l)].shape\n",
    "    grads['dW'+str(l)] = np.zeros(W_shape)\n",
    "    for i in range(W_shape[0]):\n",
    "        for j in range(W_shape[1]):\n",
    "            grads['dW'+str(l)][i, j] = x[j, 0] * sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "\n",
    "    grads['db'+str(l)] = np.zeros(b_shape)\n",
    "    for i in range(b_shape[0]):\n",
    "        grads['db'+str(l)][i, 0] = sigmoid_deriv(middleput['Z'+str(l)][i, 0]) * dA[i, 0]\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = 1\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "parameters, costs = train(epochs, batch_size, lr, train_set[:200], layer_dims=[102, 150, 60, 4], backward_func=nonvectorized_backward)\n",
    "result = test(train_set[:200], parameters)\n",
    "print('train accuracy is {:.2f}%'.format(result['accuracy']))\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 4: Vectorized Backward Propagetion<h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_backward(x, label, parameters, middleput):\n",
    "        \n",
    "    l = len(parameters) // 2\n",
    "    grads = {}\n",
    "\n",
    "    # last layer\n",
    "    dA = 2 * (middleput['A'+str(l)] - label)\n",
    "    dZ = dA * sigmoid_deriv(middleput['Z'+str(l)]) \n",
    "    grads['dW'+str(l)] = np.matmul(dZ, middleput['A'+str(l-1)].T)\n",
    "    grads['db'+str(l)] = dZ\n",
    "\n",
    "    l -= 1\n",
    "\n",
    "    # hidden layers\n",
    "    while l > 1:\n",
    "        dA = np.matmul(parameters['W'+str(l+1)].T, dZ)\n",
    "        dZ = dA * sigmoid_deriv(middleput['Z'+str(l)]) \n",
    "        grads['dW'+str(l)] = np.matmul(dZ, middleput['A'+str(l-1)].T)\n",
    "        grads['db'+str(l)] = dZ\n",
    "\n",
    "        l -= 1\n",
    "\n",
    "\n",
    "    # first layer\n",
    "    dA = np.matmul(parameters['W'+str(l+1)].T, dZ)\n",
    "    dZ = dA * sigmoid_deriv(middleput['Z'+str(l)]) \n",
    "    grads['dW'+str(l)] = np.matmul(dZ, x.T)\n",
    "    grads['db'+str(l)] = dZ   \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_train = 10\n",
    "lr = 1\n",
    "batch_size = 10\n",
    "epochs = 20\n",
    "\n",
    "total_costs = []\n",
    "total_accuracy = 0\n",
    "\n",
    "for i in tqdm(range(total_train), leave=False, bar_format='{n_fmt} / {total_fmt} {bar}'):\n",
    "    parameters, costs = train(epochs, batch_size, lr, train_set, layer_dims=[102, 150, 60, 4], backward_func=vectorized_backward)\n",
    "    total_costs.append(costs)\n",
    "    result = test(train_set, parameters)\n",
    "    total_accuracy += result['accuracy']\n",
    "    \n",
    "costs = (np.sum(np.array(total_costs), axis=0) / total_train).tolist()\n",
    "total_accuracy /= total_train\n",
    "\n",
    "print('train accuracy is {:.2f}%'.format(total_accuracy))\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase5: Final Test<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_test(total_train, epochs, batch_size, lr, train_ds, test_ds, layers_dims, lr_decay_weight=0):\n",
    "\n",
    "    total_costs = []\n",
    "    total_train_accuracy = 0\n",
    "    total_test_accuracy = 0\n",
    "\n",
    "    for i in tqdm(range(total_train), leave=False, bar_format='{n_fmt} / {total_fmt} {bar}'):\n",
    "        parameters, costs = train(epochs, batch_size, lr, train_ds, layers_dims,\n",
    "                                  backward_func=vectorized_backward, lr_decay_weight=lr_decay_weight)\n",
    "        total_costs.append(costs)\n",
    "        train_result = test(train_ds, parameters)\n",
    "        test_result = test(test_ds, parameters)\n",
    "        total_train_accuracy += train_result['accuracy']\n",
    "        total_test_accuracy += test_result['accuracy']\n",
    "\n",
    "    costs = (np.sum(np.array(total_costs), axis=0) / total_train).tolist()\n",
    "    total_train_accuracy /= total_train\n",
    "    total_test_accuracy /= total_train\n",
    "\n",
    "    print('train accuracy is {:.2f}%'.format(total_train_accuracy))\n",
    "    print('test accuracy is {:.2f}%'.format(total_test_accuracy))\n",
    "    plt.plot(costs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=10, \n",
    "                 batch_size=10, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 6: Hyperparameters Analyzing<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=15, \n",
    "                 batch_size=10, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=10, \n",
    "                 batch_size=5, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=10, \n",
    "                 batch_size=15, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=20, \n",
    "                 batch_size=10, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=40, \n",
    "                 batch_size=10, \n",
    "                 lr=.1, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=100, \n",
    "                 batch_size=10, \n",
    "                 lr=.01, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=10, \n",
    "                 batch_size=10, \n",
    "                 lr=2, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 7: Learning Rate Decay<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Decay:\n",
    "    def __init__(self, initial_lr, k):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.t = 0\n",
    "        self.k = k\n",
    "        \n",
    "    def step(self):\n",
    "        value = self.initial_lr * np.exp(-self.k * self.t)\n",
    "        self.t += 1\n",
    "        return max(value, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=30, \n",
    "                 batch_size=10, \n",
    "                 lr=1.5, \n",
    "                 train_ds=train_set, \n",
    "                 test_ds=test_set, \n",
    "                 layers_dims=[102, 150, 60, 4],\n",
    "                 lr_decay_weight=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 8: Deeper Network !!!<h2/>\n",
    "<p style='font-size: 19px'>In the last phase, 3 fruits were added to the dataset, so there are seven fruits and also we need Deeper Model !!!<p/>\n",
    "<p style='font-size: 15px'><em>warning: feel free to add more fruits to the dataset, but you should modify feature extraction a little bit and make your model more complex.<em/><p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocess Data<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Feature_extraction/train_set_features_7_fruits.pkl\", \"rb\") as f:\n",
    "    train_set_features2 = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/train_set_labels__7_fruits.pkl\", \"rb\") as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/test_set_features__7_fruits.pkl\", \"rb\") as f:\n",
    "    test_set_features2 = pickle.load(f)\n",
    "\n",
    "with open(\"Feature_extraction/test_set_labels__7_fruits.pkl\", \"rb\") as f:\n",
    "    test_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=train_set_features2, axis=0)\n",
    "train_set_features = train_set_features2[:, features_STDs > 43.4]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
    "\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=test_set_features2, axis=0)\n",
    "test_set_features = test_set_features2[:, features_STDs > 40]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "test_set_features = np.divide(test_set_features, test_set_features.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_optional = []\n",
    "test_set_optional = []\n",
    "\n",
    "for i in range(len(train_set_features)):\n",
    "    label = np.zeros((7, ), dtype='int32')\n",
    "    label[int(train_set_labels[i])] = 1\n",
    "    label = label.reshape(7,1)\n",
    "    train_set_optional.append((train_set_features[i].reshape(118,1), label))\n",
    "    \n",
    "\n",
    "for i in range(len(test_set_features)):\n",
    "    label = np.zeros((7, ), dtype='int32')\n",
    "    label[int(test_set_labels[i])] = 1\n",
    "    label = label.reshape(7,1)\n",
    "    test_set_optional.append((test_set_features[i].reshape(118,1), label))\n",
    "    \n",
    "# shuffle\n",
    "random.shuffle(train_set_optional)\n",
    "random.shuffle(test_set_optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_features.shape)\n",
    "print(train_set_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_optional[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train and Test<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_train_test(total_train=10, \n",
    "                 epochs=20, \n",
    "                 batch_size=10, \n",
    "                 lr=1, \n",
    "                 train_ds=train_set_optional, \n",
    "                 test_ds=test_set_optional, \n",
    "                 layers_dims=[118, 150, 60, 60, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
